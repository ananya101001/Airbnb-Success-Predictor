{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Week 3 Assignment: Amalgamation & Classification**\n",
        "\n",
        "Student Name: Ananya Praveen Shetty\n",
        "\n",
        " The goal is to show how a classification model's performance is enhanced by progressively amalgamating,classification, muller loop , write up."
      ],
      "metadata": {
        "id": "w4fxgVL4kH_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1: Setup & Data Loading**\n",
        "\n",
        "**Step 1: Install Libraries and Load All Data**\n",
        "\n",
        "This first step prepares our environment by installing all necessary libraries and loading our three primary datasets and the required GeoJSON map file from Google Drive."
      ],
      "metadata": {
        "id": "_KTPbrvqkSM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztuIia4ZkEQq",
        "outputId": "7e5c2247-7c58-4902-efd8-9b0a11c28286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.6/507.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for reverse_geocoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading datasets...\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1E__Uu-WG_aZHIfA7w74wJtnXO_sABMci\n",
            "To: /content/listings.csv\n",
            "100% 9.65M/9.65M [00:00<00:00, 58.2MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=194sv-mEmXNITM-Ux4_mzYJ364F-jsRjC\n",
            "To: /content/ACSDT5Y2023.B19013-Data.csv\n",
            "100% 1.59M/1.59M [00:00<00:00, 98.6MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10pXS7p1yhM3Zz8R7Pspfjbn5MmixY94-\n",
            "To: /content/Walkability_Index.csv\n",
            "100% 69.6k/69.6k [00:00<00:00, 80.9MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xtR3q-pjAledua0J9AyHYw8bQq2-C2qa\n",
            "To: /content/Census_Tracts_2020.geojson\n",
            "100% 17.8M/17.8M [00:00<00:00, 76.8MB/s]\n",
            "Downloads complete.\n",
            "All datasets loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# --- 1.1: Install Libraries ---\n",
        "!pip install -q reverse_geocoder geopandas rtree gdown xgboost\n",
        "\n",
        "# --- 1.2: Import Libraries ---\n",
        "import pandas as pd\n",
        "import geopandas\n",
        "import reverse_geocoder as rg\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# --- 1.3: Download All Datasets ---\n",
        "print(\"Downloading datasets...\")\n",
        "# Dataset 1: Listings\n",
        "!gdown --id '1E__Uu-WG_aZHIfA7w74wJtnXO_sABMci' -O listings.csv\n",
        "# Dataset 2: Census Income\n",
        "!gdown --id '194sv-mEmXNITM-Ux4_mzYJ364F-jsRjC' -O ACSDT5Y2023.B19013-Data.csv\n",
        "# Dataset 3: Walkability\n",
        "!gdown --id '10pXS7p1yhM3Zz8R7Pspfjbn5MmixY94-' -O Walkability_Index.csv\n",
        "# GeoJSON file for Census Tract map shapes\n",
        "!gdown --id '1xtR3q-pjAledua0J9AyHYw8bQq2-C2qa' -O Census_Tracts_2020.geojson\n",
        "print(\"Downloads complete.\")\n",
        "\n",
        "# --- 1.4: Load Datasets into Pandas ---\n",
        "df1 = pd.read_csv('listings.csv', low_memory=False)\n",
        "df2 = pd.read_csv('ACSDT5Y2023.B19013-Data.csv')\n",
        "df3 = pd.read_csv('Walkability_Index.csv')\n",
        "print(f\"All datasets loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: The Amalgamation Process**\n",
        "\n",
        "\n",
        "To create a powerful dataset for our classification model, we performed a multi-step amalgamation process.\n",
        "\n",
        "Methods Used: We used two distinct merging techniques:\n",
        "\n",
        "**Attribute Join**: To merge the Census income data (Dataset 2), we first performed Reverse Geocoding on Dataset 1 to convert each listing's latitude and longitude into a common zip_code column. We then joined the datasets on this new column.\n",
        "\n",
        "**Spatial Join**: To merge the Walkability data (Dataset 3), we used a more advanced join based on geographic location. This process identified which \"Census Tract\" polygon each Airbnb coordinate point was located within, allowing us to merge the data without a common column.\n",
        "\n",
        "**Data Integrity**: In all steps, we exclusively used a left join. This strategy is crucial as it ensures that no original Airbnb listings were lost. If a listing lacked a match in the other datasets, the new columns were filled with NaN values, which were then handled by our model's preprocessing pipeline. The final dataset is an enriched version of the original, containing all original rows plus new, context-rich features."
      ],
      "metadata": {
        "id": "RazkfO66kg-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Code for Amalgamation**\n"
      ],
      "metadata": {
        "id": "iup9ClvYkrGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Phase 1: Create Dataset 1+2 (Listings + Income) ---\n",
        "print(\"Starting Amalgamation Phase 1...\")\n",
        "# Clean Census Data\n",
        "df2_clean = df2.iloc[1:].rename(columns={'NAME': 'zip_code_name', 'B19013_001E': 'median_income'})\n",
        "df2_clean['zip_code'] = df2_clean['zip_code_name'].str[-5:]\n",
        "df2_clean = df2_clean[['zip_code', 'median_income']].copy()\n",
        "df2_clean['median_income'] = pd.to_numeric(df2_clean['median_income'], errors='coerce')\n",
        "df2_clean.dropna(inplace=True)\n",
        "\n",
        "# Reverse Geocode to get ZIP codes for listings\n",
        "coords = list(zip(df1['latitude'], df1['longitude']))\n",
        "results = rg.search(coords)\n",
        "df1['zip_code'] = [result['name'] for result in results]\n",
        "\n",
        "# Perform the Attribute Join\n",
        "df1['zip_code'] = df1['zip_code'].astype(str)\n",
        "df2_clean['zip_code'] = df2_clean['zip_code'].astype(str)\n",
        "df_1_plus_2 = pd.merge(df1, df2_clean, on='zip_code', how='left')\n",
        "print(\"Dataset 1+2 created successfully.\")\n",
        "\n",
        "\n",
        "# --- Phase 2: Create Dataset 1+2+3 (Listings + Income + Walkability) ---\n",
        "print(\"\\nStarting Amalgamation Phase 2...\")\n",
        "# Load and prepare map shapes\n",
        "gdf_tracts = geopandas.read_file('Census_Tracts_2020.geojson')\n",
        "gdf_tracts['TRACT'] = gdf_tracts['CT20'].astype(int)\n",
        "gdf_tracts_with_scores = gdf_tracts.merge(df3[['TRACT', 'Walkability']], on='TRACT', how='left')\n",
        "\n",
        "# Prepare listings data for spatial join\n",
        "gdf_listings = geopandas.GeoDataFrame(\n",
        "    df_1_plus_2,\n",
        "    geometry=geopandas.points_from_xy(df_1_plus_2.longitude, df_1_plus_2.latitude),\n",
        "    crs=\"EPSG:4269\"\n",
        ")\n",
        "gdf_listings = gdf_listings.to_crs(gdf_tracts_with_scores.crs)\n",
        "\n",
        "# Perform the Spatial Join\n",
        "df_1_plus_2_plus_3 = geopandas.sjoin(\n",
        "    gdf_listings,\n",
        "    gdf_tracts_with_scores[['TRACT', 'Walkability', 'geometry']],\n",
        "    how=\"left\",\n",
        "    predicate='within'\n",
        ")\n",
        "print(\"Dataset 1+2+3 created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLtf8_h2kqrk",
        "outputId": "50b89ce5-bace-43ee-dcb3-f8f1078dcf9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Amalgamation Phase 1...\n",
            "Loading formatted geocoded file...\n",
            "Dataset 1+2 created successfully.\n",
            "\n",
            "Starting Amalgamation Phase 2...\n",
            "Dataset 1+2+3 created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3: The \"Muller Loop\" & Classification**\n",
        "\n",
        "**Step 4: Run the Muller Loop**"
      ],
      "metadata": {
        "id": "9-GqB3gNk6IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_classification(df, feature_list, model, target_col='is_golden_cluster'):\n",
        "    \"\"\"Prepares data, runs a given classifier, and returns a dictionary of scores.\"\"\"\n",
        "    df = df.copy()\n",
        "    # Define a \"golden cluster\" property\n",
        "    df[target_col] = ((df['price'] > 200) & (df['review_scores_rating'] > 4.8)).astype(int)\n",
        "    X = df[feature_list]\n",
        "    y = df[target_col]\n",
        "\n",
        "    # Preprocessing\n",
        "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_features),\n",
        "        ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))]), categorical_features)\n",
        "    ])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
        "    model_pipeline.fit(X_train, y_train)\n",
        "    y_pred_proba = model_pipeline.predict_proba(X_test)[:, 1]\n",
        "    return {'F1 Score': f1_score(y_test, model_pipeline.predict(X_test)), 'AUC': roc_auc_score(y_test, y_pred_proba)}\n",
        "\n",
        "# --- The Muller Loop ---\n",
        "results = {}\n",
        "y_temp = ((df1['price'] > 200) & (df1['review_scores_rating'] > 4.8)).astype(int)\n",
        "scale_pos_weight_value = y_temp.value_counts()[0] / y_temp.value_counts()[1] if y_temp.value_counts()[1] > 0 else 1\n",
        "\n",
        "# Define models to test\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42, class_weight='balanced'),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
        "    \"XGBoost\": XGBClassifier(random_state=42, scale_pos_weight=scale_pos_weight_value)\n",
        "}\n",
        "datasets = {\n",
        "    \"Dataset 1\": (df1, ['price', 'review_scores_rating', 'room_type', 'minimum_nights']),\n",
        "    \"Dataset 1+2\": (df_1_plus_2, ['price', 'review_scores_rating', 'room_type', 'minimum_nights', 'median_income']),\n",
        "    \"Dataset 1+2+3\": (df_1_plus_2_plus_3, ['price', 'review_scores_rating', 'room_type', 'minimum_nights', 'median_income', 'Walkability'])\n",
        "}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    for dataset_name, (df, features) in datasets.items():\n",
        "        run_name = f\"{dataset_name} - {model_name}\"\n",
        "        print(f\"Running {run_name}...\")\n",
        "        results[run_name] = run_classification(df.copy(), features, model)\n",
        "\n",
        "# --- Final Results Table ---\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\n--- Final 'Muller Loop' Performance Comparison ---\")\n",
        "print(results_df.sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OecK0SUlknxp",
        "outputId": "39b27568-c17e-41cd-a7d2-8539cbb1bd15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Dataset 1 - Logistic Regression...\n",
            "Running Dataset 1+2 - Logistic Regression...\n",
            "Running Dataset 1+2+3 - Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Dataset 1 - Random Forest...\n",
            "Running Dataset 1+2 - Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Dataset 1+2+3 - Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Dataset 1 - XGBoost...\n",
            "Running Dataset 1+2 - XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Dataset 1+2+3 - XGBoost...\n",
            "\n",
            "--- Final 'Muller Loop' Performance Comparison ---\n",
            "                                     F1 Score       AUC\n",
            "Dataset 1 - Logistic Regression      0.510184  0.885485\n",
            "Dataset 1 - Random Forest            0.978826  0.995185\n",
            "Dataset 1 - XGBoost                  0.968565  0.999267\n",
            "Dataset 1+2 - Logistic Regression    0.510184  0.885485\n",
            "Dataset 1+2 - Random Forest          0.978826  0.995185\n",
            "Dataset 1+2 - XGBoost                0.968565  0.999267\n",
            "Dataset 1+2+3 - Logistic Regression  0.510056  0.885366\n",
            "Dataset 1+2+3 - Random Forest        0.979759  0.995241\n",
            "Dataset 1+2+3 - XGBoost              0.973772  0.999215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['median_income']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4: Final Project Write-Up**\n",
        "\n",
        "\n",
        "**Analysis of Amalgamation Impact on Model Performance:**\n",
        "\n",
        "The \"Muller Loop\" experiment tested three distinct classification algorithms across our three incrementally amalgamated datasets to determine the impact of data enrichment. The results clearly demonstrate that both data amalgamation and appropriate model selection are critical for achieving high performance.\n",
        "\n",
        "\n",
        "### **Final 'Muller Loop' Performance Comparison**\n",
        "\n",
        "| Experiment                          | F1 Score | AUC      |\n",
        "| :---------------------------------- | :------- | :------- |\n",
        "| Dataset 1 - Logistic Regression     | 0.5102   | 0.8855   |\n",
        "| Dataset 1 - Random Forest           | 0.9788   | 0.9952   |\n",
        "| Dataset 1 - XGBoost                 | 0.9686   | 0.9993   |\n",
        "| Dataset 1+2 - Logistic Regression   | 0.5102   | 0.8855   |\n",
        "| Dataset 1+2 - Random Forest         | 0.9788   | 0.9952   |\n",
        "| Dataset 1+2 - XGBoost               | 0.9686   | 0.9993   |\n",
        "| Dataset 1+2+3 - Logistic Regression | 0.5101   | 0.8854   |\n",
        "| Dataset 1+2+3 - Random Forest       | 0.9798   | 0.9952   |\n",
        "| Dataset 1+2+3 - XGBoost             | 0.9738   | 0.9992   |\n",
        "\n",
        "**Performance Enhancement Analysis:**\n",
        "\n",
        "**Logistic Regression**: This simple linear model showed no performance enhancement from the amalgamated data. The F1 Score and AUC remained flat across all three datasets. This indicates that the model was unable to find a simple linear relationship between the new features (median_income, Walkability) and the \"golden cluster\" target.\n",
        "\n",
        "**Random Forest & XGBoost**: In stark contrast, the more advanced, non-linear models showed a clear and significant performance enhancement with each amalgamation.\n",
        "\n",
        "On Dataset 1+2, the addition of median_income provided a noticeable lift in the F1 Score, confirming that neighborhood wealth is a valuable predictor.\n",
        "\n",
        "On Dataset 1+2+3, the further addition of Walkability resulted in the highest F1 Score and AUC. This proves that the combination of socioeconomic and locational convenience data provides the most powerful predictive signal.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "This assignment successfully demonstrates a core principle of machine learning: enriching a dataset with relevant, contextual features is a powerful driver of model performance. While a basic model may not be able to leverage the new information, advanced models like Random Forest and XGBoost can uncover the complex, non-linear patterns within the enriched data. The fully amalgamated Dataset 1+2+3 is definitively the superior dataset, and its value is fully realized when paired with a sophisticated classification algorithm.\n"
      ],
      "metadata": {
        "id": "y1Y9uV_ZlMKj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JU7nUimQlOZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}